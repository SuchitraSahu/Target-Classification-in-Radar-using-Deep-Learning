{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all the required libraries\n",
    "\n",
    "import numpy as np\n",
    "import array as arr\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import wave\n",
    "import pylab\n",
    "from pathlib import Path\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import random\n",
    "from tensorflow.keras import regularizers\n",
    "import librosa\n",
    "import librosa.display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths to input and output data\n",
    "\n",
    "INPUT_DIR = r'E:\\00 PS-1\\Project\\imp\\Training audio samples'\n",
    "OUTPUT_DIR = r'E:\\00 PS-1\\Project\\imp'\n",
    "# OUTPUT_DIR --> Where you want to save the spectrogram images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print names of 5 WAV files from the input path\n",
    "\n",
    "parent_list = os.listdir(INPUT_DIR)\n",
    "for i in range(5):\n",
    "    print(parent_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot first 3 WAV files as a waveform and a frequency spectrum\n",
    "\n",
    "for i in range(3): \n",
    "    signal_wave = wave.open(os.path.join(INPUT_DIR, parent_list[i]), 'r')\n",
    "    sample_rate = 16000\n",
    "    sig = np.frombuffer(signal_wave.readframes(sample_rate), dtype=np.int16)\n",
    "\n",
    "    plt.figure(figsize=(12,12))\n",
    "    plot_a = plt.subplot(211)\n",
    "    plot_a.set_title(parent_list[i])\n",
    "    plot_a.plot(sig)\n",
    "    plot_a.set_xlabel('sample rate * time')\n",
    "    plot_a.set_ylabel('energy')\n",
    "\n",
    "    plot_b = plt.subplot(212)\n",
    "    plot_b.specgram(sig, NFFT=1024, Fs=sample_rate, noverlap=900)\n",
    "    plot_b.set_xlabel('Time')\n",
    "    plot_b.set_ylabel('Frequency')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Spectrograms for the audio samples and saving them\n",
    "\n",
    "\n",
    "# Utility function to get sound and frame rate info\n",
    "np.seterr(divide = 'ignore') \n",
    "def get_wav_info(wav_file):\n",
    "    wav = wave.open(wav_file, 'r')\n",
    "    frames = wav.readframes(-1)\n",
    "    sound_info = pylab.frombuffer(frames, 'int16')\n",
    "    frame_rate = wav.getframerate()\n",
    "    wav.close()\n",
    "    return sound_info, frame_rate\n",
    "\n",
    "# For every recording, make a spectogram and save it as label_class_no.png\n",
    "if not os.path.exists(os.path.join(OUTPUT_DIR, 'audio-images')):\n",
    "    os.mkdir(os.path.join(OUTPUT_DIR, 'audio-images'))\n",
    "    \n",
    "for filename in os.listdir(INPUT_DIR):\n",
    "    if \"wav\" in filename:\n",
    "        file_path = os.path.join(INPUT_DIR, filename)\n",
    "        file_stem = Path(file_path).stem\n",
    "        target_dir = f'class_{file_stem[:2]}'\n",
    "        dist_dir = os.path.join(os.path.join(OUTPUT_DIR, 'audio-images'), target_dir)\n",
    "        file_dist_path = os.path.join(dist_dir, file_stem)\n",
    "        if not os.path.exists(file_dist_path + '.png'):\n",
    "            if not os.path.exists(dist_dir):\n",
    "                os.mkdir(dist_dir)\n",
    "            file_stem = Path(file_path).stem\n",
    "            sound_info, frame_rate = get_wav_info(file_path)\n",
    "            pylab.specgram(sound_info, Fs=frame_rate)\n",
    "            pylab.savefig(f'{file_dist_path}.png')\n",
    "            pylab.close()\n",
    "\n",
    "# Print the ten classes in our dataset\n",
    "path_list = os.listdir(os.path.join(OUTPUT_DIR, 'audio-images'))\n",
    "print(\"Classes: \\n\")\n",
    "for i in range(16):\n",
    "    print(path_list[i])\n",
    "    \n",
    "# File names for class 1\n",
    "path_list = os.listdir(os.path.join(OUTPUT_DIR, 'audio-images/class_1_'))\n",
    "print(\"\\nA few example files: \\n\")\n",
    "for i in range(16):\n",
    "    print(path_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare constants\n",
    "IMAGE_HEIGHT = 256\n",
    "IMAGE_WIDTH = 256\n",
    "BATCH_SIZE = 32\n",
    "N_CHANNELS = 3\n",
    "N_CLASSES = 16\n",
    "\n",
    "# Make a dataset containing the training spectrograms\n",
    "train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "                                             batch_size=BATCH_SIZE,\n",
    "                                             validation_split=0.2,\n",
    "                                             directory=os.path.join(OUTPUT_DIR, 'audio-images'),\n",
    "                                             shuffle=True,\n",
    "                                             color_mode='rgb',\n",
    "                                             image_size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n",
    "                                             subset=\"training\",\n",
    "                                             seed=0)\n",
    "\n",
    "# Make a dataset containing the validation spectrogram\n",
    "valid_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "                                             batch_size=BATCH_SIZE,\n",
    "                                             validation_split=0.2,\n",
    "                                             directory=os.path.join(OUTPUT_DIR, 'audio-images'),\n",
    "                                             shuffle=True,\n",
    "                                             color_mode='rgb',\n",
    "                                             image_size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n",
    "                                             subset=\"validation\",\n",
    "                                             seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 12))\n",
    "for images, labels in train_dataset.take(1):\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        plt.title(int(labels[i]))\n",
    "        plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to prepare our datasets for modelling + Data Augmentation\n",
    "\n",
    "def prepare(ds, augment=False):\n",
    "    # Define our one transformation\n",
    "    rescale = tf.keras.Sequential([tf.keras.layers.experimental.preprocessing.Rescaling(1./255)])\n",
    "    flip_and_rotate = tf.keras.Sequential([\n",
    "        tf.keras.layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n",
    "        tf.keras.layers.experimental.preprocessing.RandomRotation(0.2)\n",
    "    ])\n",
    "    \n",
    "    # Apply rescale to both datasets and augmentation only to training\n",
    "    ds = ds.map(lambda x, y: (rescale(x, training=True), y))\n",
    "    if augment: ds = ds.map(lambda x, y: (flip_and_rotate(x, training=True), y))\n",
    "    return ds\n",
    "\n",
    "train_dataset = prepare(train_dataset, augment=False)\n",
    "valid_dataset = prepare(valid_dataset, augment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CNN model\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Input(shape=(IMAGE_HEIGHT, IMAGE_WIDTH, N_CHANNELS)))\n",
    "#model.add(tf.keras.layers.Conv2D(32, 3, strides=1, padding='same', activation='relu'))\n",
    "#model.add(tf.keras.layers.BatchNormalization())\n",
    "#model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "#model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Conv2D(64, 3, strides=2, padding='same', activation='relu'))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Conv2D(128, 3, strides=2, padding='same', activation='relu'))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "# REGULARIZATION\n",
    "# Add Dropout layer\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "# Add L2 regularization to the dense layers\n",
    "model.add(tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(N_CLASSES, activation='softmax'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=tf.keras.optimizers.RMSprop(), metrics=['accuracy'])\n",
    "\n",
    "# Train model for 100 epochs, capture the history\n",
    "from keras import callbacks\n",
    "earlystopping = callbacks.EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5, restore_best_weights=True)\n",
    "history = model.fit(train_dataset, epochs=100, validation_data=valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss curves for training and validation.\n",
    "\n",
    "history_dict = history.history\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values = history_dict['val_loss']\n",
    "epochs = range(1, len(loss_values)+1)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(epochs, loss_values, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the accuracy curves for training and validation.\n",
    "\n",
    "acc_values = history_dict['accuracy']\n",
    "val_acc_values = history_dict['val_accuracy']\n",
    "epochs = range(1, len(acc_values)+1)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(epochs, acc_values, 'bo', label='Training accuracy')\n",
    "plt.plot(epochs, val_acc_values, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the final loss and accuracy\n",
    "\n",
    "final_loss, final_acc = model.evaluate(valid_dataset, verbose=0)\n",
    "print(\"Final loss: {0:.3f}, final accuracy: {1:.3f}\".format(final_loss, final_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test dataset\n",
    "TST_DIR = (r'E:\\00 PS-1\\Project\\imp\\Test Audio')\n",
    "TES_DIR = (r'E:\\00 PS-1\\Project\\imp')\n",
    "\n",
    "np.seterr(divide = 'ignore') \n",
    "def get_wav_info(wav_file):\n",
    "    wav = wave.open(wav_file, 'r')\n",
    "    frames = wav.readframes(-1)\n",
    "    sound_info = pylab.frombuffer(frames, 'int16')\n",
    "    frame_rate = wav.getframerate()\n",
    "    wav.close()\n",
    "    return sound_info, frame_rate\n",
    "\n",
    "# For every recording, make a spectogram and save it as label_class_no.png\n",
    "if not os.path.exists(os.path.join(TES_DIR, 'Test Dataset')):\n",
    "    os.mkdir(os.path.join(TES_DIR, 'Test Dataset'))\n",
    "    \n",
    "for filename in os.listdir(TST_DIR):\n",
    "    if \"wav\" in filename:\n",
    "        file_path = os.path.join(TST_DIR, filename)\n",
    "        file_stem = Path(file_path).stem\n",
    "        target_dir = f'class_{file_stem[:2]}'\n",
    "        dist_dir = os.path.join(os.path.join(TES_DIR, 'Test Dataset'), target_dir)\n",
    "        file_dist_path = os.path.join(dist_dir, file_stem)\n",
    "        if not os.path.exists(file_dist_path + '.png'):\n",
    "            if not os.path.exists(dist_dir):\n",
    "                os.mkdir(dist_dir)\n",
    "            file_stem = Path(file_path).stem\n",
    "            sound_info, frame_rate = get_wav_info(file_path)\n",
    "            pylab.specgram(sound_info, Fs=frame_rate)\n",
    "            pylab.savefig(f'{file_dist_path}.png')\n",
    "            pylab.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)  # Apply necessary preprocessing\n",
    "TEST_DIR = (r'E:\\00 PS-1\\Project\\imp\\Test Dataset')\n",
    "image_height = IMAGE_HEIGHT\n",
    "image_width = IMAGE_WIDTH\n",
    "test_dataset = test_datagen.flow_from_directory(\n",
    "    directory=TEST_DIR,\n",
    "    target_size=(image_width, image_height),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=None,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(test_dataset)\n",
    "\n",
    "# Convert predictions to class labels (if needed)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Get the class labels of the test dataset\n",
    "class_labels = test_dataset.class_indices\n",
    "\n",
    "# Invert the class labels dictionary\n",
    "inv_class_labels = {v: k for k, v in class_labels.items()}\n",
    "\n",
    "# Convert predicted labels to class names\n",
    "predicted_classes = [inv_class_labels[label] for label in predicted_labels]\n",
    "\n",
    "# Print the predictions\n",
    "for i in range(len(predicted_classes)):\n",
    "    print(f\"Image {i+1}: {predicted_classes[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "\n",
    "def browse_file():\n",
    "    global sample_dir  # Declare the global variable to be modified inside the function\n",
    "    sample_dir = filedialog.askopenfilename()\n",
    "    if sample_dir:\n",
    "        print(\"Selected File:\", sample_dir)\n",
    "\n",
    "def create_gui():\n",
    "    root = tk.Tk()\n",
    "    root.title(\"File Explorer GUI\")\n",
    "    root.geometry(\"300x100\")\n",
    "\n",
    "    # Create a button to open the file explorer\n",
    "    browse_button = tk.Button(root, text=\"Open File Explorer\", command=browse_file)\n",
    "    browse_button.pack(pady=10)\n",
    "\n",
    "    # Create a button to close the popup\n",
    "    close_button = tk.Button(root, text=\"Close\", command = root.destroy)\n",
    "    close_button.pack(pady=10)\n",
    "\n",
    "    root.mainloop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_gui()\n",
    "\n",
    "print('sample_dir = ',sample_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing for one particular sample\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)  # Apply necessary preprocessing\n",
    "TEST_DIR = (r'E:\\00 PS-1\\Project\\imp\\Test Dataset')\n",
    "image_height = IMAGE_HEIGHT\n",
    "image_width = IMAGE_WIDTH\n",
    "test_dataset = test_datagen.flow_from_directory(\n",
    "    directory=TEST_DIR,\n",
    "    target_size=(image_width, image_height),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=None,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Path to that sample (Copy path and paste here if required)\n",
    "#sample_image_path = (r'F:\\00 PS-1\\Project\\imp\\Test Dataset\\class_1\\GP 250.png')\n",
    "\n",
    "# Load the sample image\n",
    "sample_image_path = sample_dir\n",
    "sample_image = tf.keras.preprocessing.image.load_img(sample_image_path, target_size=(IMAGE_WIDTH, IMAGE_HEIGHT))\n",
    "sample_image_array = tf.keras.preprocessing.image.img_to_array(sample_image)\n",
    "sample_image_array = np.expand_dims(sample_image_array, axis=0)\n",
    "sample_image_array = sample_image_array / 255.0  # Normalize the image\n",
    "\n",
    "# Make prediction for the sample image\n",
    "prediction = model.predict(sample_image_array)\n",
    "predicted_label = np.argmax(prediction)\n",
    "\n",
    "# Get the class labels of the test dataset\n",
    "class_labels = test_dataset.class_indices\n",
    "\n",
    "# Invert the class labels dictionary\n",
    "inv_class_labels = {v: k for k, v in class_labels.items()}\n",
    "\n",
    "# Convert predicted label to class name\n",
    "predicted_class = inv_class_labels[predicted_label]\n",
    "\n",
    "# Print the prediction for the sample image\n",
    "print(f\"Sample Image: {sample_image_path}\")\n",
    "print(f\"Prediction: {predicted_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import style\n",
    "import tkinter as tk\n",
    "from tkinter import ttk\n",
    "from tkinter import *\n",
    "from PIL import Image, ImageTk\n",
    "\n",
    "LARGE_FONT= (\"Helvetica\", 20)\n",
    "NORM_FONT= (\"Helvetica\", 12)\n",
    "SMALL_FONT= (\"Helvetica\", 10)\n",
    "\n",
    "if predicted_class == \"class_0_\":\n",
    "   output = \"Single Man or Crwaling Men at 100m\"\n",
    "   image_path = Image.open(r'F:\\00 PS-1\\Project\\imp\\popup images\\0_Single Man.gif')\n",
    "\n",
    "if predicted_class == \"class_1_\":\n",
    "   output = \"Group of Men at 100m\"\n",
    "   image_path = Image.open(r'F:\\00 PS-1\\Project\\imp\\popup images\\1_Group of Men.gif')\n",
    "\n",
    "if predicted_class == \"class_2_\":\n",
    "   output = \"Group of Men at 150m\"\n",
    "   image_path = Image.open(r'F:\\00 PS-1\\Project\\imp\\popup images\\1_Group of Men.gif')\n",
    "\n",
    "if predicted_class == \"class_3_\":\n",
    "   output = \"Group of Men at 200m\"\n",
    "   image_path = Image.open(r'F:\\00 PS-1\\Project\\imp\\popup images\\1_Group of Men.gif')\n",
    "\n",
    "if predicted_class == \"class_4_\":\n",
    "   output = \"Group of Men at 250m\"\n",
    "   image_path = Image.open(r'F:\\00 PS-1\\Project\\imp\\popup images\\1_Group of Men.gif')\n",
    "\n",
    "if predicted_class == \"class_5_\":\n",
    "   output = \"Group of Men at 300m\"\n",
    "   image_path = Image.open(r'F:\\00 PS-1\\Project\\imp\\popup images\\1_Group of Men.gif')\n",
    "\n",
    "if predicted_class == \"class_6_\":\n",
    "   output = \"Heavy Vehicle at 150m\"\n",
    "   image_path = Image.open(r'F:\\00 PS-1\\Project\\imp\\popup images\\3_Heavy Vehicle.gif')\n",
    "\n",
    "if predicted_class == \"class_7_\":\n",
    "   output = \"Heavy Vehicle at 200m\"\n",
    "   image_path = Image.open(r'F:\\00 PS-1\\Project\\imp\\popup images\\3_Heavy Vehicle.gif')\n",
    "\n",
    "if predicted_class == \"class_8_\":\n",
    "   output = \"Heavy Vehicle at 250m\"\n",
    "   image_path = Image.open(r'F:\\00 PS-1\\Project\\imp\\popup images\\3_Heavy Vehicle.gif')\n",
    "\n",
    "if predicted_class == \"class_9_\":\n",
    "   output = \"Heavy Vehicle at 300m\"\n",
    "   image_path = Image.open(r'F:\\00 PS-1\\Project\\imp\\popup images\\3_Heavy Vehicle.gif')\n",
    "\n",
    "if predicted_class == \"class_10\":\n",
    "   output = \"Light Vehicle at 100m\"\n",
    "   image_path = Image.open(r'F:\\00 PS-1\\Project\\imp\\popup images\\2_Light Vehicle.gif')\n",
    "\n",
    "if predicted_class == \"class_11\":\n",
    "   output = \"Light Vehicle at 200m\"\n",
    "   image_path = Image.open(r'F:\\00 PS-1\\Project\\imp\\popup images\\2_Light Vehicle.gif')\n",
    "\n",
    "if predicted_class == \"class_12\":\n",
    "   output = \"Light Vehicle at 300m\"\n",
    "   image_path = Image.open(r'F:\\00 PS-1\\Project\\imp\\popup images\\2_Light Vehicle.gif')\n",
    "\n",
    "if predicted_class == \"class_13\":\n",
    "   output = \"Tank at 200m\"\n",
    "   image_path = Image.open(r'F:\\00 PS-1\\Project\\imp\\popup images\\4_Tank.gif')\n",
    "\n",
    "if predicted_class == \"class_14\":\n",
    "   output = \"Tank at 400m\"\n",
    "   image_path = Image.open(r'F:\\00 PS-1\\Project\\imp\\popup images\\4_Tank.gif')\n",
    "\n",
    "if predicted_class == \"class_15\":\n",
    "   output = \"Tank at 500m\"\n",
    "   image_path = Image.open(r'F:\\00 PS-1\\Project\\imp\\popup images\\4_Tank.gif')\n",
    "\n",
    "popup = tk.Tk()\n",
    "popup.title(\"Target Info\")\n",
    "popup.geometry(\"400x400\")\n",
    "\n",
    "output_label = tk.Label(popup, text=output, font=NORM_FONT)\n",
    "output_label.pack()\n",
    "\n",
    "# Create a photoimage object of the image in the path\n",
    "image1 = Image.open(image_path)\n",
    "image1 = image1.resize((350, 350), Image.LANCZOS)\n",
    "test = ImageTk.PhotoImage(image1)\n",
    "label1 = tk.Label(popup, image=test)\n",
    "label1.image = test\n",
    "label1.pack(pady=10)\n",
    "    \n",
    "popup.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the output dir (the saved spectrogram images)\n",
    "# Don't do this unless you are changing the training dataset\n",
    "\n",
    "import shutil\n",
    "shutil.rmtree('/kaggle/working/audio-images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some extra pieces of code that we were experimenting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import splitfolders\n",
    "# To only split into training and validation set, set a tuple to `ratio`, i.e, `(.8, .2)`.\n",
    "splitfolders.ratio((r'E:\\00 PS-1\\Project\\imp\\audio-images/'), output=(r'E:\\00 PS-1\\Project\\imp'), seed=1337, ratio=(.8, .2)) # default values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255, # rescale all pixel values from 0-255, so after this step all our pixel values are in range (0,1)\n",
    "        shear_range=0.2, #to apply some random tranfromations\n",
    "        zoom_range=0.2, #to apply zoom\n",
    "        horizontal_flip=True) # image will be flipper horiztest_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "        rescale=1./255, # rescale all pixel values from 0-255, so aftre this step all our pixel values are in range (0,1)\n",
    "        shear_range=0.2, #to apply some random tranfromations\n",
    "        zoom_range=0.2, #to apply zoom\n",
    "        horizontal_flip=True) # image will be flipper horiztest_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare constants\n",
    "\n",
    "IMAGE_HEIGHT = 256\n",
    "IMAGE_WIDTH = 256\n",
    "BATCH_SIZE = 32\n",
    "N_CHANNELS = 3\n",
    "N_CLASSES = 16\n",
    "\n",
    "# Make a dataset containing the training spectrograms\n",
    "train_dataset = train_datagen.flow_from_directory(\n",
    "        (r'E:\\00 PS-1\\Project\\imp\\imp Datasets\\train'),\n",
    "        target_size=(256, 256),\n",
    "        batch_size=32,\n",
    "        class_mode='sparse',\n",
    "        shuffle = False)\n",
    "\n",
    "# Make a dataset containing the validation spectrogram\n",
    "valid_dataset = test_datagen.flow_from_directory(\n",
    "        (r'E:\\00 PS-1\\Project\\imp\\imp Datasets\\val'),\n",
    "        target_size=(256, 256),\n",
    "        batch_size=32,\n",
    "        class_mode='sparse',\n",
    "        shuffle = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an array train_labels\n",
    "\n",
    "TR_DIR = (r'E:\\00 PS-1\\Project\\imp\\imp Datasets\\train/')\n",
    "tr_labels = []\n",
    "for filename in os.listdir(TR_DIR):\n",
    "    if \"wav\" in filename:\n",
    "        file_path = os.path.join(TR_DIR, filename)\n",
    "        file_stem = Path(file_path).stem\n",
    "        tr_labels.append(file_stem[:2])\n",
    "print(tr_labels)\n",
    "\n",
    "train_labels = np.array(tr_labels)\n",
    "print(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA AUGMENTATION\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Create an instance of the ImageDataGenerator\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Generate augmented data\n",
    "augmented_train_dataset = datagen.flow(train_dataset, train_labels, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMETER TUNING\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# Define a function to create the CNN model\n",
    "def create_model(optimizer='adam', dropout_rate=0.2):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    # Define your model architecture here\n",
    "    # ...\n",
    "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create the KerasClassifier wrapper\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "# Define the hyperparameters and their values to search\n",
    "param_grid = {\n",
    "    'optimizer': ['adam', 'rmsprop'],\n",
    "    'dropout_rate': [0.2, 0.3, 0.4]\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3)\n",
    "grid_result = grid.fit(train_dataset, train_labels)\n",
    "\n",
    "# Print the best hyperparameters and corresponding accuracy\n",
    "print(\"Best Hyperparameters: \", grid_result.best_params_)\n",
    "print(\"Best Accuracy: \", grid_result.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMETER TUNING 2\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# Define a function to create the CNN model\n",
    "def create_model(optimizer='adam', dropout_rate=0.2):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    # Define your model architecture here\n",
    "    # ...\n",
    "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create the KerasClassifier wrapper\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "# Define the hyperparameters and their values to search\n",
    "param_grid = {\n",
    "    'optimizer': ['adam', 'rmsprop'],\n",
    "    'dropout_rate': [0.2, 0.3, 0.4]\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3)\n",
    "grid_result = grid.fit(train_dataset)\n",
    "\n",
    "# Print the best hyperparameters and corresponding accuracy\n",
    "print(\"Best Hyperparameters: \", grid_result.best_params_)\n",
    "print(\"Best Accuracy: \", grid_result.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LEARNING RATE\n",
    "\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "# Define a learning rate schedule function\n",
    "\n",
    "def lr_schedule(epoch):\n",
    "    initial_learning_rate = 0.01\n",
    "    decay = 0.1\n",
    "    lr = initial_learning_rate * decay ** (epoch // 10)\n",
    "    return lr\n",
    "\n",
    "# Use the learning rate schedule during training\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "model.fit(train_dataset, train_labels, epochs=100, callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define the image data generator for training data\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Set the path to your train dataset directory\n",
    "#train_directory = train_dataset\n",
    "\n",
    "# Set the target image size and batch size\n",
    "image_width, image_height = 256, 256\n",
    "batch_size = 32\n",
    "\n",
    "# Extract the train labels\n",
    "train_labels = train_dataset.classes\n",
    "\n",
    "# Print the train labels\n",
    "print(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define test set and training set\n",
    "test_set = valid_dataset\n",
    "training_set = train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define the parameters for data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "image_width = 256\n",
    "image_height = 256\n",
    "\n",
    "# Generate the augmented training set\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    r'E:\\00 PS-1\\Project\\imp\\audio-images',  # Path to the training data directory\n",
    "    target_size=(image_width, image_height),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Access the class indices\n",
    "class_indices = train_generator.class_indices\n",
    "print(class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba\n",
    "wav_file=(r'E:\\00 PS-1\\Project\\Samples -30 June 2023\\New folder\\DOPPLER COLLECTION\\GP 2 MEN CRAWLING 200M\\0 DEGREE APPROCH\\200A0.wav')\n",
    "#preprocess the audio file\n",
    "wav =  librosa.load(wav_file, res_type='kaiser_fast')\n",
    "\n",
    "# sound_info, frame_rate = get_wav_info(file_path)\n",
    "specgram_features = pylab.specgram(wav)\n",
    "specgram_scaled_features = np.mean(specgram_features.T,axis=0)\n",
    "#Reshape MFCC feature to 2-D array\n",
    "specgram_scaled_features=specgram_scaled_features.reshape(1,-1)\n",
    "predicted_label=model.predict_classes(specgram_features)\n",
    "x_predict=model.predict(specgram_scaled_features) \n",
    "predicted_label=np.argmax(x_predict,axis=1)\n",
    "print(predicted_label)\n",
    "prediction_class = labelencoder.inverse_transform(predicted_label) \n",
    "print(prediction_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_set.reset()\n",
    "pred = model.predict_generator(test_set, steps=50, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class_indices=np.argmax(pred,axis=1)\n",
    "\n",
    "labels = (training_set.class_indices)\n",
    "labels = dict((v,k) for k,v in labels.items())\n",
    "predictions = [labels[k] for k in predicted_class_indices]\n",
    "predictions = predictions[:200]\n",
    "filenames=test_set.filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(filename, len(predictions)))\n",
    "# (200, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=pd.DataFrame({\"Filename\":filenames,\n",
    "                      \"Predictions\":predictions},orient='index')\n",
    "results.to_csv(\"prediction_results.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
