{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all the required libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import wave\n",
    "import pylab\n",
    "from pathlib import Path\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import random\n",
    "import keras\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths to input and output data\n",
    "\n",
    "INPUT_DIR = r'F:\\00 PS-1\\Project\\imp\\Training audio samples'\n",
    "OUTPUT_DIR = r'F:\\00 PS-1\\Project\\imp'\n",
    "# OUTPUT_DIR --> Where you want to save the spectrogram images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print names of 5 WAV files from the input path\n",
    "\n",
    "parent_list = os.listdir(INPUT_DIR)\n",
    "for i in range(5):\n",
    "    print(parent_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot first 3 WAV files as a waveform and a frequency spectrum\n",
    "\n",
    "for i in range(3): \n",
    "    signal_wave = wave.open(os.path.join(INPUT_DIR, parent_list[i]), 'r')\n",
    "    sample_rate = 16000\n",
    "    sig = np.frombuffer(signal_wave.readframes(sample_rate), dtype=np.int16)\n",
    "\n",
    "    plt.figure(figsize=(12,12))\n",
    "    plot_a = plt.subplot(211)\n",
    "    plot_a.set_title(parent_list[i])\n",
    "    plot_a.plot(sig)\n",
    "    plot_a.set_xlabel('sample rate * time')\n",
    "    plot_a.set_ylabel('energy')\n",
    "\n",
    "    plot_b = plt.subplot(212)\n",
    "    plot_b.specgram(sig, NFFT=1024, Fs=sample_rate, noverlap=900)\n",
    "    plot_b.set_xlabel('Time')\n",
    "    plot_b.set_ylabel('Frequency')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To plot spectrograms for each audio sample and save them.\n",
    "\n",
    "# Utility function to get sound and frame rate info\n",
    "def get_wav_info(wav_file):\n",
    "    wav = wave.open(wav_file, 'r')\n",
    "    frames = wav.readframes(-1)\n",
    "    sound_info = pylab.frombuffer(frames, 'int16')\n",
    "    frame_rate = wav.getframerate()\n",
    "    wav.close()\n",
    "    return sound_info, frame_rate\n",
    "\n",
    "# For every recording, make a spectogram and save it as label_speaker_no.png\n",
    "if not os.path.exists(os.path.join(OUTPUT_DIR, 'audio-images')):\n",
    "    os.mkdir(os.path.join(OUTPUT_DIR, 'audio-images'))\n",
    "    \n",
    "for filename in os.listdir(INPUT_DIR):\n",
    "    if \"wav\" in filename:\n",
    "        file_path = os.path.join(INPUT_DIR, filename)\n",
    "        file_stem = Path(file_path).stem\n",
    "        target_dir = f'class_{file_stem[0]}'\n",
    "        dist_dir = os.path.join(os.path.join(OUTPUT_DIR, 'audio-images'), target_dir)\n",
    "        file_dist_path = os.path.join(dist_dir, file_stem)\n",
    "        if not os.path.exists(file_dist_path + '.png'):\n",
    "            if not os.path.exists(dist_dir):\n",
    "                os.mkdir(dist_dir)\n",
    "            file_stem = Path(file_path).stem\n",
    "            sound_info, frame_rate = get_wav_info(file_path)\n",
    "            pylab.specgram(sound_info, Fs=frame_rate)\n",
    "            pylab.savefig(f'{file_dist_path}.png')\n",
    "            pylab.close()\n",
    "\n",
    "# Print the ten classes in our dataset\n",
    "path_list = os.listdir(os.path.join(OUTPUT_DIR, 'audio-images'))\n",
    "print(\"Classes: \\n\")\n",
    "for i in range(4):\n",
    "    print(path_list[i])\n",
    "    \n",
    "# File names for class 1\n",
    "path_list = os.listdir(os.path.join(OUTPUT_DIR, 'audio-images/class_1'))\n",
    "print(\"\\nA few example files: \\n\")\n",
    "for i in range(10):\n",
    "    print(path_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare constants\n",
    "IMAGE_HEIGHT = 256\n",
    "IMAGE_WIDTH = 256\n",
    "BATCH_SIZE = 32\n",
    "N_CHANNELS = 3\n",
    "N_CLASSES = 4\n",
    "\n",
    "# Make a dataset containing the training spectrograms\n",
    "train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "                                             batch_size=BATCH_SIZE,\n",
    "                                             validation_split=0.2,\n",
    "                                             directory=os.path.join(OUTPUT_DIR, 'audio-images'),\n",
    "                                             shuffle=True,\n",
    "                                             color_mode='rgb',\n",
    "                                             image_size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n",
    "                                             subset=\"training\",\n",
    "                                             seed=0)\n",
    "\n",
    "# Make a dataset containing the validation spectrogram\n",
    "valid_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "                                             batch_size=BATCH_SIZE,\n",
    "                                             validation_split=0.2,\n",
    "                                             directory=os.path.join(OUTPUT_DIR, 'audio-images'),\n",
    "                                             shuffle=True,\n",
    "                                             color_mode='rgb',\n",
    "                                             image_size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n",
    "                                             subset=\"validation\",\n",
    "                                             seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 12))\n",
    "for images, labels in train_dataset.take(1):\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        plt.title(int(labels[i]))\n",
    "        plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to prepare our datasets for modelling\n",
    "def prepare(ds, augment=False):\n",
    "    # Define our one transformation\n",
    "    rescale = tf.keras.Sequential([tf.keras.layers.experimental.preprocessing.Rescaling(1./255)])\n",
    "    flip_and_rotate = tf.keras.Sequential([\n",
    "        tf.keras.layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n",
    "        tf.keras.layers.experimental.preprocessing.RandomRotation(0.2)\n",
    "    ])\n",
    "    \n",
    "    # Apply rescale to both datasets and augmentation only to training\n",
    "    ds = ds.map(lambda x, y: (rescale(x, training=True), y))\n",
    "    if augment: ds = ds.map(lambda x, y: (flip_and_rotate(x, training=True), y))\n",
    "    return ds\n",
    "\n",
    "train_dataset = prepare(train_dataset, augment=False)\n",
    "valid_dataset = prepare(valid_dataset, augment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from keras import callbacks\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CNN model\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Input(shape=(IMAGE_HEIGHT, IMAGE_WIDTH, N_CHANNELS)))\n",
    "#model.add(tf.keras.layers.Conv2D(32, 3, strides=1, padding='same', activation='relu'))\n",
    "#model.add(tf.keras.layers.BatchNormalization())\n",
    "#model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "#model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Conv2D(64, 3, strides=2, padding='same', activation='relu'))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu'))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "# REGULARIZATION\n",
    "# Add Dropout layer\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "# Add L2 regularization to the dense layers\n",
    "model.add(tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(N_CLASSES, activation='softmax'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=tf.keras.optimizers.RMSprop(), metrics=['accuracy'])\n",
    "\n",
    "# Train model for 100 epochs, capture the history\n",
    "earlystopping = callbacks.EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5, restore_best_weights=True)\n",
    "history = model.fit(train_dataset, epochs=100, validation_data=valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss curves for training and validation.\n",
    "history_dict = history.history\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values = history_dict['val_loss']\n",
    "epochs = range(1, len(loss_values)+1)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(epochs, loss_values, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the accuracy curves for training and validation.\n",
    "acc_values = history_dict['accuracy']\n",
    "val_acc_values = history_dict['val_accuracy']\n",
    "epochs = range(1, len(acc_values)+1)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(epochs, acc_values, 'bo', label='Training accuracy')\n",
    "plt.plot(epochs, val_acc_values, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the final loss and accuracy\n",
    "final_loss, final_acc = model.evaluate(valid_dataset, verbose=0)\n",
    "print(\"Final loss: {0:.3f}, final accuracy: {1:.3f}\".format(final_loss, final_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test dataset\n",
    "TST_DIR = (r'F:\\00 PS-1\\Project\\imp\\Test Audio')\n",
    "TES_DIR = (r'F:\\00 PS-1\\Project\\imp')\n",
    "\n",
    "np.seterr(divide = 'ignore') \n",
    "def get_wav_info(wav_file):\n",
    "    wav = wave.open(wav_file, 'r')\n",
    "    frames = wav.readframes(-1)\n",
    "    sound_info = pylab.frombuffer(frames, 'int16')\n",
    "    frame_rate = wav.getframerate()\n",
    "    wav.close()\n",
    "    return sound_info, frame_rate\n",
    "\n",
    "# For every recording, make a spectogram and save it as label_class_no.png\n",
    "if not os.path.exists(os.path.join(TES_DIR, 'Test Dataset')):\n",
    "    os.mkdir(os.path.join(TES_DIR, 'Test Dataset'))\n",
    "    \n",
    "for filename in os.listdir(TST_DIR):\n",
    "    if \"wav\" in filename:\n",
    "        file_path = os.path.join(TST_DIR, filename)\n",
    "        file_stem = Path(file_path).stem\n",
    "        target_dir = f'class_{file_stem[0]}'\n",
    "        dist_dir = os.path.join(os.path.join(TES_DIR, 'Test Dataset'), target_dir)\n",
    "        file_dist_path = os.path.join(dist_dir, file_stem)\n",
    "        if not os.path.exists(file_dist_path + '.png'):\n",
    "            if not os.path.exists(dist_dir):\n",
    "                os.mkdir(dist_dir)\n",
    "            file_stem = Path(file_path).stem\n",
    "            sound_info, frame_rate = get_wav_info(file_path)\n",
    "            pylab.specgram(sound_info, Fs=frame_rate)\n",
    "            pylab.savefig(f'{file_dist_path}.png')\n",
    "            pylab.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)  # Apply necessary preprocessing\n",
    "TEST_DIR = (r'F:\\00 PS-1\\Project\\imp\\Test Dataset')\n",
    "image_height = IMAGE_HEIGHT\n",
    "image_width = IMAGE_WIDTH\n",
    "test_dataset = test_datagen.flow_from_directory(\n",
    "    directory=TEST_DIR,\n",
    "    target_size=(image_width, image_height),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=None,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(test_dataset)\n",
    "\n",
    "# Convert predictions to class labels (if needed)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Get the class labels of the test dataset\n",
    "class_labels = test_dataset.class_indices\n",
    "\n",
    "# Invert the class labels dictionary\n",
    "inv_class_labels = {v: k for k, v in class_labels.items()}\n",
    "\n",
    "# Convert predicted labels to class names\n",
    "predicted_classes = [inv_class_labels[label] for label in predicted_labels]\n",
    "\n",
    "# Print the predictions\n",
    "for i in range(len(predicted_classes)):\n",
    "    print(f\"Image {i+1}: {predicted_classes[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "\n",
    "def browse_file():\n",
    "    global sample_dir  # Declare the global variable to be modified inside the function\n",
    "    sample_dir = filedialog.askopenfilename()\n",
    "    if sample_dir:\n",
    "        print(\"Selected File:\", sample_dir)\n",
    "\n",
    "def create_gui():\n",
    "    root = tk.Tk()\n",
    "    root.title(\"File Explorer GUI\")\n",
    "    root.geometry(\"300x100\")\n",
    "\n",
    "    # Create a button to open the file explorer\n",
    "    browse_button = tk.Button(root, text=\"Open File Explorer\", command=browse_file)\n",
    "    browse_button.pack(pady=10)\n",
    "\n",
    "    # Create a button to close the popup\n",
    "    close_button = tk.Button(root, text=\"Close\", command = root.destroy)\n",
    "    close_button.pack(pady=10)\n",
    "\n",
    "    root.mainloop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_gui()\n",
    "\n",
    "print('sample_dir = ',sample_dir)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing for one particular sample\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)  # Apply necessary preprocessing\n",
    "TEST_DIR = (r'F:\\00 PS-1\\Project\\imp\\Test Dataset')\n",
    "image_height = IMAGE_HEIGHT\n",
    "image_width = IMAGE_WIDTH\n",
    "test_dataset = test_datagen.flow_from_directory(\n",
    "    directory=TEST_DIR,\n",
    "    target_size=(image_width, image_height),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=None,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Path to that sample (...)\n",
    "#sample_image_path = (r'F:\\00 PS-1\\Project\\imp\\Test Dataset\\class_2\\LV 100.png')\n",
    "\n",
    "# Load the sample image\n",
    "sample_image_path = sample_dir\n",
    "sample_image = tf.keras.preprocessing.image.load_img(sample_image_path, target_size=(IMAGE_WIDTH, IMAGE_HEIGHT))\n",
    "sample_image_array = tf.keras.preprocessing.image.img_to_array(sample_image)\n",
    "sample_image_array = np.expand_dims(sample_image_array, axis=0)\n",
    "sample_image_array = sample_image_array / 255.0  # Normalize the image\n",
    "\n",
    "# Make prediction for the sample image\n",
    "prediction = model.predict(sample_image_array)\n",
    "predicted_label = np.argmax(prediction)\n",
    "\n",
    "# Get the class labels of the test dataset\n",
    "class_labels = test_dataset.class_indices\n",
    "\n",
    "# Invert the class labels dictionary\n",
    "inv_class_labels = {v: k for k, v in class_labels.items()}\n",
    "\n",
    "# Convert predicted label to class name\n",
    "predicted_class = inv_class_labels[predicted_label]\n",
    "\n",
    "# Print the prediction for the sample image\n",
    "print(f\"Sample Image: {sample_image_path}\")\n",
    "print(f\"Prediction: {predicted_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import style\n",
    "import tkinter as tk\n",
    "from tkinter import ttk\n",
    "from tkinter import *\n",
    "from PIL import Image, ImageTk\n",
    "\n",
    "LARGE_FONT= (\"Helvetica\", 20)\n",
    "NORM_FONT= (\"Helvetica\", 14)\n",
    "SMALL_FONT= (\"Helvetica\", 10)\n",
    "\n",
    "if predicted_class == \"class_0\":\n",
    "   output = \"Single Man or Crawling Men\"\n",
    "   image_path = (r'F:\\00 PS-1\\Project\\imp\\popup images\\0_Single Man.gif')\n",
    "\n",
    "elif predicted_class == \"class_1\":\n",
    "   output = \"Group of Men\"\n",
    "   image_path = (r'F:\\00 PS-1\\Project\\imp\\popup images\\1_Group of Men.gif')\n",
    "\n",
    "elif predicted_class == \"class_2\":\n",
    "   output = \"Heavy Vehicle\"\n",
    "   image_path = (r'F:\\00 PS-1\\Project\\imp\\popup images\\3_Heavy Vehicle.gif')\n",
    "\n",
    "elif predicted_class == \"class_3\":\n",
    "   output = \"Light Vehicle\"\n",
    "   image_path = (r'F:\\00 PS-1\\Project\\imp\\popup images\\2_Light Vehicle.gif')\n",
    "\n",
    "\n",
    "popup = tk.Tk()\n",
    "popup.title(\"Target Info\")\n",
    "popup.geometry(\"400x400\")\n",
    "\n",
    "output_label = tk.Label(popup, text=output, font=NORM_FONT)\n",
    "output_label.pack()\n",
    "\n",
    "# Create a photoimage object of the image in the path\n",
    "image1 = Image.open(image_path)\n",
    "image1 = image1.resize((350, 350), Image.LANCZOS)\n",
    "test = ImageTk.PhotoImage(image1)\n",
    "label1 = tk.Label(popup, image=test)\n",
    "label1.image = test\n",
    "label1.pack(pady=10)\n",
    "    \n",
    "popup.mainloop()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the output dir (To be used when the training dataset needs to be changed)\n",
    "import shutil\n",
    "shutil.rmtree('/kaggle/working/audio-images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some extra code snippets we used: (Some extra ideas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR2 = r'F:\\00 PS-1\\Project\\imp\\2nd order'\n",
    "OUTPUT_DIR3 = r'F:\\00 PS-1\\Project\\imp\\audio-images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to get sound and frame rate info\n",
    "def get_wav_info(wav_file):\n",
    "    wav = wave.open(wav_file, 'r')\n",
    "    frames = wav.readframes(-1)\n",
    "    sound_info = pylab.frombuffer(frames, 'int16')\n",
    "    frame_rate = wav.getframerate()\n",
    "    wav.close()\n",
    "    return sound_info, frame_rate\n",
    "\n",
    "# For every recording, make a spectogram and save it as label_speaker_no.png\n",
    "if not os.path.exists(os.path.join(OUTPUT_DIR2, 'audio-images')):\n",
    "    os.mkdir(os.path.join(OUTPUT_DIR2, 'audio-images'))\n",
    "    \n",
    "for filename in os.listdir(INPUT_DIR):\n",
    "    if \"wav\" in filename:\n",
    "        file_path = os.path.join(INPUT_DIR, filename)\n",
    "        file_stem = Path(file_path).stem\n",
    "        target_dir = f'class_{file_stem[:3]}'\n",
    "        dist_dir = os.path.join(os.path.join(OUTPUT_DIR2, 'audio-images'), target_dir)\n",
    "        file_dist_path = os.path.join(dist_dir, file_stem)\n",
    "        if not os.path.exists(file_dist_path + '.png'):\n",
    "            if not os.path.exists(dist_dir):\n",
    "                os.mkdir(dist_dir)\n",
    "            file_stem = Path(file_path).stem\n",
    "            sound_info, frame_rate = get_wav_info(file_path)\n",
    "            pylab.specgram(sound_info, Fs=frame_rate)\n",
    "            pylab.savefig(f'{file_dist_path}.png')\n",
    "            pylab.close()\n",
    "\n",
    "# Print the ten classes in our dataset\n",
    "path_list = os.listdir(os.path.join(OUTPUT_DIR2, 'audio-images'))\n",
    "print(\"Classes: \\n\")\n",
    "for i in range(16):\n",
    "    print(path_list[i])\n",
    "    \n",
    "# File names for class 1\n",
    "path_list = os.listdir(os.path.join(OUTPUT_DIR2, 'audio-images/class_1_1'))\n",
    "print(\"\\nA few example files: \\n\")\n",
    "for i in range(10):\n",
    "    print(path_list[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
